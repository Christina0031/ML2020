{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw4_RNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"r67y9UpchZ38"},"source":["# Recurrent Neural Networks\n","\n","本次作业是要让同学接触 NLP 当中一个简单的 task —— 语句分类（文本分类）\n","\n","给定一个语句，判断他有没有恶意（负面标 1，正面标 0）\n","\n","若有任何问题，欢迎来信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com\n","\n","参考资料\n","\n","http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","\n","https://www.cnblogs.com/zyb993963526/p/13784199.html\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"9ajS_WskRo0S","executionInfo":{"status":"ok","timestamp":1614999719685,"user_tz":-480,"elapsed":929,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}}},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# path_prefix = 'drive/My Drive/Colab Notebooks/hw4 - Recurrent Neural Network'\n","path_prefix = './'"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9YrAlczfM_w6"},"source":["### Download Dataset\n","有三个档案，分别是 training_label.txt、training_nolabel.txt、testing_data.txt\n","\n","- training_label.txt：有 label 的 training data（句子配上 0 or 1，+++$+++ 只是分隔符，不要理它）\n","    - e.g., 1 +++$+++ are wtf ... awww thanks !\n","\n","- training_nolabel.txt：没有 label 的 training data（只有句子），用来做 semi-supervised learning\n","    - ex: hates being this burnt !! ouch\n","\n","- testing_data.txt：你要判断 testing data 里面的句子是 0 or 1\n","\n","    >id,text\n","\n","    >0,my dog ate our dinner . no , seriously ... he ate it .\n","\n","    >1,omg last day sooon n of primary noooooo x im gona be swimming out of school wif the amount of tears am gona cry\n","\n","    >2,stupid boys .. they ' re so .. stupid !"]},{"cell_type":"code","metadata":{"id":"sv-l0aiK4fqI","executionInfo":{"status":"ok","timestamp":1614999720210,"user_tz":-480,"elapsed":1447,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}}},"source":["# from google.colab import drive\r\n","# drive.mount('/content/drive')\r\n","# path_prefix = 'drive/My Drive/Colab Notebooks/hw4 - Recurrent Neural Network'\r\n","path_prefix = './'"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"x2gwKORmuViJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615000300441,"user_tz":-480,"elapsed":581673,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}},"outputId":"a86d6732-c432-4fcc-df5d-db63c28cb13e"},"source":["# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1dPHIl8ZnfDz_fxNd2ZeBYedTat2lfxcO' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/training_label.txt'\n","# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1x1rJOX_ETqnOZjdMAbEE2pqIjRNa8xcc' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/training_nolabel.txt'\n","# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=16CtnQwSDCob9xmm6EdHHR7PNFNiOrQ30' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/testing_data.txt'\n","\n","!gdown --id '1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8' --output data.zip\n","!unzip data.zip\n","!ls"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8\n","To: /content/data.zip\n","45.1MB [00:00, 109MB/s] \n","Archive:  data.zip\n","replace training_label.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n","  inflating: training_label.txt      \n","  inflating: testing_data.txt        \n","  inflating: training_nolabel.txt    \n","ckpt.model  predict.csv  testing_data.txt    training_nolabel.txt\n","data.zip    sample_data  training_label.txt  w2v_all.model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8hDIokoP6464","executionInfo":{"status":"ok","timestamp":1615000300443,"user_tz":-480,"elapsed":581670,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}}},"source":["# this is for filtering the warnings\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fc143hSvNGr6"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"ICDIhhgCY2-M","executionInfo":{"status":"ok","timestamp":1615000300444,"user_tz":-480,"elapsed":581666,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}}},"source":["# utils.py\n","# 这个 block 用来先定义一些等等常用到的函式\n","import torch\n","import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def load_training_data(path='training_label.txt'):\n","    # 把 training 时需要的 data 读进来\n","    # 如果是 'training_label.txt'，需要读取 label，如果是 'training_nolabel.txt'，不需要读取 label\n","    # return: x -- list of 每一个句子的所有单词list ; y -- list of label\n","    if 'training_label' in path:\n","        with open(path, 'r') as f:\n","            lines = f.readlines()\n","            lines = [line.strip('\\n').split(' ') for line in lines]\n","        x = [line[2:] for line in lines]\n","        y = [line[0] for line in lines]\n","        return x, y\n","    else:\n","        with open(path, 'r') as f:\n","            lines = f.readlines()\n","            x = [line.strip('\\n').split(' ') for line in lines]\n","        return x\n","\n","def load_testing_data(path='testing_data'):\n","    # 把 testing 时需要的 data 读进来\n","    # return: x -- list of 每一个句子的所有单词list\n","    with open(path, 'r') as f:\n","        lines = f.readlines()\n","        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n","        X = [sen.split(' ') for sen in X]\n","    return X\n","\n","def evaluation(outputs, labels):\n","    # outputs => probability (float)\n","    # labels => labels \n","    # return correct number\n","    outputs[outputs>=0.5] = 1 # 大于等于 0.5 为正面\n","    outputs[outputs<0.5] = 0 # 小于 0.5 为负面\n","    correct = torch.sum(torch.eq(outputs, labels)).item()\n","    return correct"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oYE8UYQsNIxM"},"source":["### Train Word to Vector"]},{"cell_type":"code","metadata":{"id":"cgGWaF8_2S3q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615000599147,"user_tz":-480,"elapsed":880363,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}},"outputId":"572d8887-b091-4b38-edc1-9557cd280192"},"source":["# w2v.py\n","# 这个 block 是用来训练 word to vector 的 word embedding\n","# 注意！这个 block 在训练 word to vector 时是用 cpu，可能要花到 10 分钟以上\n","import os\n","import numpy as np\n","import pandas as pd\n","import argparse\n","from gensim.models import word2vec\n","\n","def train_word2vec(x):\n","    # 训练 word to vector 的 word embedding\n","    # Dimensionality of the word vectors = 250\n","    # return model\n","    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)\n","    return model\n","\n","if __name__ == \"__main__\":\n","    print(\"loading training data ...\")\n","    train_x, y = load_training_data('training_label.txt')\n","    train_x_no_label = load_training_data('training_nolabel.txt')\n","\n","    print(\"loading testing data ...\")\n","    test_x = load_testing_data('testing_data.txt')\n","\n","    #model = train_word2vec(train_x + train_x_no_label + test_x)\n","    model = train_word2vec(train_x + test_x)\n","    \n","    print(\"saving model ...\")\n","    # model.save(os.path.join(path_prefix, 'model/w2v_all.model'))\n","    model.save(os.path.join(path_prefix, 'w2v_all.model'))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["loading training data ...\n","loading testing data ...\n","saving model ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3wHLtS0wNR6w"},"source":["### Data Preprocess"]},{"cell_type":"code","metadata":{"id":"CfGKiOitk5ob","executionInfo":{"status":"ok","timestamp":1615000880513,"user_tz":-480,"elapsed":818,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}}},"source":["# preprocess.py\n","# 这个 block 用来做 data 的预处理\n","from torch import nn\n","from gensim.models import Word2Vec\n","\n","class Preprocess():\n","    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n","        self.w2v_path = w2v_path\n","        self.sentences = sentences  # 单词二维数组\n","        self.sen_len = sen_len\n","        self.idx2word = []  # [word, word...]\n","        self.word2idx = {}  # {word: id}\n","        self.embedding_matrix = [] # [ vector , vector , ...]\n","    def get_w2v_model(self):\n","        # 把之前训练好的 word to vec 模型读进来\n","        self.embedding = Word2Vec.load(self.w2v_path) # 获取某个单词的词向量 embedding['word']\n","        self.embedding_dim = self.embedding.vector_size\n","    def add_embedding(self, word):\n","        # 把 word 加进 embedding，并赋予他一个随机生成的 representation vector\n","        # word 只会是 \"<PAD>\" 或 \"<UNK>\"\n","        vector = torch.empty(1, self.embedding_dim)\n","        torch.nn.init.uniform_(vector)\n","        self.word2idx[word] = len(self.word2idx)\n","        self.idx2word.append(word)\n","        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], dim=0)\n","    def make_embedding(self, load=True):\n","        print(\"Get embedding ...\")\n","        # 取得训练好的 Word2vec word embedding\n","        if load:\n","            print(\"loading word to vec model ...\")\n","            self.get_w2v_model()\n","        else:\n","            raise NotImplementedError\n","        # 制作一个 word2idx 的 dictionary\n","        # 制作一个 idx2word 的 list\n","        # 制作一个 word2vector 的 list\n","        for i, word in enumerate(self.embedding.wv.vocab):\n","            print('get words #{}'.format(i+1), end='\\r')\n","            #e.g. self.word2index['he'] = 1 \n","            #e.g. self.index2word[1] = 'he'\n","            #e.g. self.vectors[1] = 'he' vector\n","            self.word2idx[word] = len(self.word2idx)\n","            self.idx2word.append(word)\n","            self.embedding_matrix.append(self.embedding[word])\n","        print('')\n","        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n","        # 将 \"<PAD>\" 跟 \"<UNK>\" 加进 embedding 里面\n","        self.add_embedding(\"<PAD>\")\n","        self.add_embedding(\"<UNK>\")\n","        print(\"total words: {}\".format(len(self.embedding_matrix)))\n","        print(\"embedding_matrix：\", self.embedding_matrix.shape)\n","        return self.embedding_matrix\n","        # 返回所有单词word2vector 的 list\n","    def pad_sequence(self, sentence):\n","        # 将每个句子变成一样的长度\n","        if len(sentence) > self.sen_len:\n","            sentence = sentence[:self.sen_len]\n","        else:\n","            pad_len = self.sen_len - len(sentence)\n","            for _ in range(pad_len):\n","                sentence.append(self.word2idx[\"<PAD>\"])\n","        assert len(sentence) == self.sen_len\n","        return sentence\n","    def sentence_word2idx(self):\n","        # return 二维张量 把句子里面的字转成相对应的 index\n","        sentence_list = []\n","        for i, sen in enumerate(self.sentences):\n","            print('sentence count #{}'.format(i+1), end='\\r')\n","            sentence_idx = []\n","            for word in sen:\n","                if (word in self.word2idx.keys()):\n","                    sentence_idx.append(self.word2idx[word])\n","                else:\n","                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n","            sentence_idx = self.pad_sequence(sentence_idx) # 将每个句子变成一样的长度\n","            sentence_list.append(sentence_idx)\n","        return torch.LongTensor(sentence_list)\n","    def labels_to_tensor(self, y):\n","        # 把 labels 转成 tensor\n","        y = [int(label) for label in y]\n","        return torch.LongTensor(y)\n"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3WJB7go5NWL0"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"id":"XketwKs4lFfB","executionInfo":{"status":"ok","timestamp":1615000880513,"user_tz":-480,"elapsed":813,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}}},"source":["# data.py\n","# 实作 dataset 所需要的 '__init__', '__getitem__', '__len__'\n","# 好让 dataloader 能使用\n","import torch\n","from torch.utils import data\n","\n","class TwitterDataset(data.Dataset):\n","    \"\"\"\n","    Expected data shape like:(data_num, data_len)\n","    Data can be a list of numpy array or a list of lists\n","    input data shape : (data_num, seq_len, feature_dim)\n","    \n","    __len__ will return the number of data\n","    \"\"\"\n","    def __init__(self, X, y):\n","        self.data = X\n","        self.label = y\n","    def __getitem__(self, idx):\n","        if self.label is None: return self.data[idx]\n","        return self.data[idx], self.label[idx]\n","    def __len__(self):\n","        return len(self.data)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uNJ8xWIMNa2r"},"source":["### Model"]},{"cell_type":"code","metadata":{"id":"ZS6RJADulIq1","executionInfo":{"status":"ok","timestamp":1615000880514,"user_tz":-480,"elapsed":810,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}}},"source":["# model.py\n","# 这个 block 是要拿来训练的模型\n","import torch\n","from torch import nn\n","class LSTM_Net(nn.Module):\n","    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n","        super(LSTM_Net, self).__init__()\n","        # 制作 embedding layer\n","        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n","        self.embedding.weight = torch.nn.Parameter(embedding) # 使用之前训练的embedding weight\n","        # 是否将 embedding fix 住，如果 fix_embedding 为 False，在训练过程中，embedding 也会跟着被训练\n","        self.embedding.weight.requires_grad = False if fix_embedding else True\n","        self.embedding_dim = embedding.size(1)\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.dropout = dropout\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        # LSTM输入格式 注意batch_fist 参见下网站\n","        # https://pytorch.org/tutorials/beginner/chatbot_tutorial.html#prepare-data-for-models\n","        self.classifier = nn.Sequential( nn.Dropout(dropout),\n","                                         nn.Linear(hidden_dim, 1),\n","                                         nn.Sigmoid() )\n","    def forward(self, inputs):\n","        #input dimension： torch.Size([128, 20])\n","        inputs = self.embedding(inputs)\n","        #inputs dimension： torch.Size([128, 20, 250])\n","        x, _ = self.lstm(inputs, None)\n","        # x 的 dimension (batch, seq_len, hidden_size)\n","        #x dimension： torch.Size([128, 20, 150])\n","        # 取用 LSTM 最后一层的 hidden state\n","        # batch, seq, feature\n","        # 最后一个序列最后一层输出的output就是最后一层的 hidden state\n","        x = x[:, -1, :] \n","        x = self.classifier(x)\n","        return x"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWlpEL0sNc10"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"4QR4MMz-lR7i","executionInfo":{"status":"ok","timestamp":1615000881264,"user_tz":-480,"elapsed":1557,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}}},"source":["# train.py\n","# 这个 block 是用来训练模型的\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n","    total = sum(p.numel() for p in model.parameters())  # 返回数组中元素的个数\n","    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n","    model.train() # 将 model 的模式设为 train，这样 optimizer 就可以更新 model 的参数\n","\n","    criterion = nn.BCELoss() # 定义损失函数，这里我们使用 binary cross entropy loss\n","    t_batch = len(train) \n","    v_batch = len(valid) \n","    optimizer = optim.Adam(model.parameters(), lr=lr) # 将模型的参数给 optimizer，并给予适当的 learning rate\n","    total_loss, total_acc, best_acc = 0, 0, 0\n","    for epoch in range(n_epoch):\n","        total_loss, total_acc = 0, 0\n","        # 这段做 training\n","        for i, (inputs, labels) in enumerate(train):\n","            inputs = inputs.to(device, dtype=torch.long) \n","            # device 为 \"cuda\"，将 inputs 转成 torch.cuda.LongTensor\n","            labels = labels.to(device, dtype=torch.float) \n","            # device为 \"cuda\"，将 labels 转成 torch.cuda.FloatTensor，因为等等要喂进 criterion，所以型态要是 float\n","            optimizer.zero_grad() \n","            # 由于 loss.backward() 的 gradient 会累加，所以每次喂完一个 batch 后需要归零\n","            outputs = model(inputs) # 将 input 喂给模型\n","            outputs = outputs.squeeze() \n","            # squeeze（）函数可以删除数组形状中的单维度条目，即把shape中为1的维度去掉，但是对非单维的维度不起作用。\n","            # 去掉最外面的 dimension，好让 outputs 可以喂进 criterion()\n","            loss = criterion(outputs, labels) # 计算此时模型的 training loss\n","            loss.backward() # 算 loss 的 gradient\n","            optimizer.step() # 更新训练模型的参数\n","            correct = evaluation(outputs, labels) # 计算此时模型的 training accuracy\n","            total_acc += (correct / batch_size)\n","            total_loss += loss.item()\n","            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n","            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n","        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n","\n","        # 这段做 validation\n","        model.eval() # 将 model 的模式设为 eval，这样 model 的参数就会固定住\n","        with torch.no_grad():\n","            total_loss, total_acc = 0, 0\n","            for i, (inputs, labels) in enumerate(valid):\n","                inputs = inputs.to(device, dtype=torch.long) \n","                # device 为 \"cuda\"，将 inputs 转成 torch.cuda.LongTensor\n","                labels = labels.to(device, dtype=torch.float) \n","                # device 为 \"cuda\"，将 labels 转成 torch.cuda.FloatTensor，因为等等要喂进 criterion，所以型态要是 float\n","                outputs = model(inputs) # 将 input 喂给模型\n","                outputs = outputs.squeeze() # 去掉最外面的 dimension，好让 outputs 可以喂进 criterion()\n","                loss = criterion(outputs, labels) # 计算此时模型的 validation loss\n","                correct = evaluation(outputs, labels) # 计算此时模型的 validation accuracy\n","                total_acc += (correct / batch_size)\n","                total_loss += loss.item()\n","\n","            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n","            if total_acc > best_acc:\n","                # 如果 validation 的结果优于之前所有的结果，就把当下的模型存下来以备之后做预测时使用\n","                best_acc = total_acc\n","                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n","                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n","                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n","        print('-----------------------------------------------')\n","        model.train() # 将 model 的模式设为 train，这样 optimizer 就可以更新 model 的参数（因为刚刚转成 eval 模式）"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qF5YQrupNfCS"},"source":["### Test"]},{"cell_type":"code","metadata":{"id":"2X2wkdAYxHYA","executionInfo":{"status":"ok","timestamp":1615000881265,"user_tz":-480,"elapsed":1555,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}}},"source":["# test.py\n","# 这个 block 用来对 testing_data.txt 做预测\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def testing(batch_size, test_loader, model, device):\n","    model.eval()\n","    ret_output = []\n","    with torch.no_grad():\n","        for i, inputs in enumerate(test_loader):\n","            inputs = inputs.to(device, dtype=torch.long)\n","            outputs = model(inputs)\n","            outputs = outputs.squeeze()\n","            outputs[outputs>=0.5] = 1 # 大于等于 0.5 为正面\n","            outputs[outputs<0.5] = 0 # 小于 0.5 为负面\n","            # outputs是Tensor并且为float，转化\n","            ret_output += outputs.int().tolist()\n","    return ret_output"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dfnKj0KXNeoz"},"source":["### Main"]},{"cell_type":"code","metadata":{"id":"EztIWqCmlZof","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615000953361,"user_tz":-480,"elapsed":73648,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}},"outputId":"8ce18a4c-0a36-4a16-8ae2-ee5043272c0d"},"source":["# main.py\n","import os\n","import torch\n","import argparse\n","import numpy as np\n","from torch import nn\n","from gensim.models import word2vec\n","from sklearn.model_selection import train_test_split\n","\n","# 通过 torch.cuda.is_available() 的回传值进行判断是否有使用 GPU 的环境，如果有的话 device 就设为 \"cuda\"，没有的话就设为 \"cpu\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 处理好各个 data 的路径\n","train_with_label = os.path.join(path_prefix, 'training_label.txt')\n","train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n","testing_data = os.path.join(path_prefix, 'testing_data.txt')\n","\n","w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 处理 word to vec model 的路径\n","\n","# 定义句子长度、要不要固定 embedding、batch 大小、要训练几个 epoch、learning rate 的值、model 的文件夹路径\n","sen_len = 20\n","fix_embedding = True # fix embedding during training\n","batch_size = 128\n","epoch = 5\n","lr = 0.001\n","# model_dir = os.path.join(path_prefix, 'model/') # model directory for checkpoint model\n","model_dir = path_prefix # model directory for checkpoint model\n","\n","print(\"loading data ...\") # 把 'training_label.txt' 跟 'training_nolabel.txt' 读进来\n","train_x, y = load_training_data(train_with_label)\n","train_x_no_label = load_training_data(train_no_label)\n","\n","# 对 input 跟 labels 做预处理\n","preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n","embedding = preprocess.make_embedding(load=True)\n","train_x = preprocess.sentence_word2idx()\n","y = preprocess.labels_to_tensor(y)\n","\n","# 制作一个 model 的对象\n","model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n","model = model.to(device) # device为 \"cuda\"，model 使用 GPU 来训练（喂进去的 inputs 也需要是 cuda tensor）\n","\n","# 把 data 分为 training data 跟 validation data（将一部份 training data 拿去当作 validation data）\n","X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]\n","print(\"X_train.shape: \",X_train.shape)\n","\n","# 把 data 做成 dataset 供 dataloader 取用\n","train_dataset = TwitterDataset(X=X_train, y=y_train)\n","val_dataset = TwitterDataset(X=X_val, y=y_val)\n","\n","# 把 data 转成 batch of tensors\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = True,\n","                                            num_workers = 8)\n","\n","val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = False,\n","                                            num_workers = 8)\n","\n","# 开始训练\n","training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["loading data ...\n","Get embedding ...\n","loading word to vec model ...\n","get words #24694\n","total words: 24696\n","embedding_matrix： torch.Size([24696, 250])\n","torch.Size([180000, 20])\n","\n","start training, parameter total:6415351, trainable:241351\n","\n","\n","Train | Loss:0.49792 Acc: 75.013\n","Valid | Loss:0.46203 Acc: 78.100 \n","saving model with acc 78.100\n","-----------------------------------------------\n","\n","Train | Loss:0.44426 Acc: 79.047\n","Valid | Loss:0.44646 Acc: 78.717 \n","saving model with acc 78.717\n","-----------------------------------------------\n","\n","Train | Loss:0.42734 Acc: 80.114\n","Valid | Loss:0.43530 Acc: 79.563 \n","saving model with acc 79.563\n","-----------------------------------------------\n","\n","Train | Loss:0.41528 Acc: 80.802\n","Valid | Loss:0.43161 Acc: 79.518 \n","-----------------------------------------------\n","\n","Train | Loss:0.40362 Acc: 81.491\n","Valid | Loss:0.42182 Acc: 80.001 \n","saving model with acc 80.001\n","-----------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8fQeaQNeNm3L"},"source":["### Predict and Write to csv file"]},{"cell_type":"code","metadata":{"id":"vFvjFQopxVrt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615000981260,"user_tz":-480,"elapsed":101543,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}},"outputId":"cea6baa3-ec46-4e85-bc1b-7d16b881c4ba"},"source":["# 开始测试模型并做预测\n","print(\"loading testing data ...\")\n","test_x = load_testing_data(testing_data)\n","preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n","embedding = preprocess.make_embedding(load=True)\n","test_x = preprocess.sentence_word2idx()\n","test_dataset = TwitterDataset(X=test_x, y=None)\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = False,\n","                                            num_workers = 8)\n","print('\\nload model ...')\n","model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n","outputs = testing(batch_size, test_loader, model, device)\n","\n","# 写到 csv 档案供上传 Kaggle\n","tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n","print(\"save csv ...\")\n","tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n","print(\"Finish Predicting\")\n","\n","# 以下是使用 command line 上传到 Kaggle 的方式\n","# 需要先 pip install kaggle、Create API Token，详细请看 https://github.com/Kaggle/kaggle-api 以及 https://www.kaggle.com/code1110/how-to-submit-from-google-colab\n","# kaggle competitions submit [competition-name] -f [csv file path]] -m [message]\n","# e.g., kaggle competitions submit ml-2020spring-hw4 -f output/predict.csv -m \"......\""],"execution_count":30,"outputs":[{"output_type":"stream","text":["loading testing data ...\n","Get embedding ...\n","loading word to vec model ...\n","get words #24694\n","total words: 24696\n","embedding_matrix： torch.Size([24696, 250])\n","sentence count #200000\n","load model ...\n","save csv ...\n","Finish Predicting\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qSvgTRuGu2Qb"},"source":["#### Check where the files are"]},{"cell_type":"code","metadata":{"id":"8SZYJQ62utiK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615000981754,"user_tz":-480,"elapsed":102034,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}},"outputId":"0be8457f-993b-4eec-b563-9cf64ed19a93"},"source":["!pwd\n","!ls"],"execution_count":31,"outputs":[{"output_type":"stream","text":["/content\n","ckpt.model  predict.csv  testing_data.txt    training_nolabel.txt\n","data.zip    sample_data  training_label.txt  w2v_all.model\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uZf3E2O1wsQo"},"source":["#### Download the files to your computer"]},{"cell_type":"code","metadata":{"id":"pzsAmmRUwqdA","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1615000981755,"user_tz":-480,"elapsed":102032,"user":{"displayName":"Hongwei Ji","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhvQT7fWFl-Ih5t3pOszju6SRiuK6Bjf-pZQS6G=s64","userId":"13890997035374529817"}},"outputId":"86d3b763-95e8-4dc4-f6d3-3f710a86da6b"},"source":["from google.colab import files\n","files.download('predict.csv')"],"execution_count":32,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_cb7c2ef0-9cfa-46c6-928e-25d57126f19c\", \"predict.csv\", 1688899)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]}]}