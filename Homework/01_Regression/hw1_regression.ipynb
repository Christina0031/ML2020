{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9FfatPz6MU3"
   },
   "source": [
    "# **Homework 1: Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsoaNwrZA0ui"
   },
   "source": [
    "本次目标：由前 9 个小时的 18 个 features (包含 PM2.5)预测的 10 个小时的 PM2.5。<!-- 可以参考 <link> 获知更细项的作业说明。-->\n",
    "\n",
    "<!-- 首先，从 https://drive.google.com/open?id=1El0zvTkrSuqCTDcMpijXpADvJzZC2Jpa 将整个文件夹下载下来，并将下载下来的文件夹放到自己的 Google Drive（注意：上传到自己 Google Drive 的是文件夹 hw1-regression，而非压缩文件） -->\n",
    "\n",
    "\n",
    "若有任何问题，欢迎来信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7RiAkkjCc6l"
   },
   "source": [
    "# **Load 'train.csv'**\n",
    "train.csv 的资料为 12 个月中，每个月取 20 天，每天 24 小时的数据(每小时数据有 18 个 features)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AfNX-hB3kN8"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# data = pd.read_csv('gdrive/My Drive/hw1-regression/train.csv', header = None, encoding = 'big5')\n",
    "# 读取事先存放好的Training data\n",
    "data = pd.read_csv('./train.csv', encoding = 'big5')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqUdj00pDTpo"
   },
   "source": [
    "# **Preprocessing** \n",
    "取需要的数值部分，将 'RAINFALL' 字段全部补 0。\n",
    "另外，如果要在 colab 重复这段程序代码的执行，请从头开始执行(把上面的都重新跑一次)，以避免跑出不是自己要的结果（若自己写程序不会遇到，但 colab 重复跑这段会一直往下取资料。意即第一次取原本数据的第三栏之后的数据，第二次取第一次取的数据掉三栏之后的数据，...）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIGP7XUYD_Yb"
   },
   "outputs": [],
   "source": [
    "# 我们只需要第三列以后的data\n",
    "data = data.iloc[:,3:]\n",
    "# 将NR的数据全部置为0\n",
    "data[data == 'NR'] = 0\n",
    "# 将dataframe幻化成numpy\n",
    "raw_data = data.to_numpy()\n",
    "\n",
    "#查看raw_data\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7PCrVwX6jBF"
   },
   "source": [
    "# **Extract Features (1)**\n",
    "![图片说明](https://drive.google.com/uc?id=1LyaqD4ojX07oe5oDzPO99l9ts5NRyArH)\n",
    "![图片说明](https://drive.google.com/uc?id=1ZroBarcnlsr85gibeqEF-MtY13xJTG47)\n",
    "\n",
    "将原始 4320(=18 * 12 * 20) * 18 的资料依照每个月分重组成 12 个 18 (features) * 480 (hours) 的资料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBnrGYXu9dZQ"
   },
   "outputs": [],
   "source": [
    "# 现在要开始处理Training data\n",
    "# 由于提供的数据是每个月前20天(每天24小时)的资料(共18种类型)，因此每个月的这20*24=480个小时的时间是连续的，每10个小时可以作为一笔data，480小时共471笔data\n",
    "# 由于不同月份之间的时间是不连续的，因此先把所有的资料按照月份分开\n",
    "month_data = {}\n",
    "for month in range(12):\n",
    "    # 每个月共20*24=480份数据，共18种空气成分，每一种成分每月都有480份数据，因此初始化一个18*480的array\n",
    "    sample = np.empty([18, 480])\n",
    "    for day in range(20):\n",
    "        # sample中加入第day天的数据，由于每天都有24份数据共24列,故列的范围是24*day~24*(day+1)；选择加入sample的是第20×month+day这一天的数据，由于每天有18种大气成分的数据共18行，因此行的范围是18*(20*month+day)~18*(20*month+day+1)\n",
    "        sample[:, day * 24 : (day + 1) * 24] = raw_data[18 * (20 * month + day) : 18 * (20 * month + day + 1), :]\n",
    "    # 把这个月的data放进month_data[month]里去\n",
    "    month_data[month] = sample\n",
    "\n",
    "# 查看第一个月的数据，并借此查看数据的结构：每一行都是一种大气成分在24*20个连续小时内的值\n",
    "month_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhVmtFEQ9D6t"
   },
   "source": [
    "# **Extract Features (2)**\n",
    "![alt text](https://drive.google.com/uc?id=1wKoPuaRHoX682LMiBgIoOP4PDyNKsJLK)\n",
    "![alt text](https://drive.google.com/uc?id=1FRWWiXQ-Qh0i9tyx0LiugHYF_xDdkhLN)\n",
    "\n",
    "每个月会有 480hrs，每 9 小时形成一个 data，每个月会有 471 个 data，故总资料数为 471 * 12 笔，而每笔 data 有 9 * 18 的 features (一小时 18 个 features * 9 小时)。\n",
    "\n",
    "对应的 target 则有 471 * 12 个(第 10 个小时的 PM2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcOrC4Fi-n3i"
   },
   "outputs": [],
   "source": [
    "# 这个时候我们就要在12段连续的时间里每10个小时取出一笔Training data\n",
    "# 每个月连续时间有20*24=480h，每10h作为一笔data，共可以分为471笔data；一共12个月，因此共12*471笔data\n",
    "# 每一笔data，前9h为input，最后1h的PM2.5为output，因此input是一个18*9的矩阵，如果把它摊平就是一个18*9的feature vector\n",
    "\n",
    "# x是input，共12*471笔data，每个input是一个18*9的feature vector\n",
    "x = np.empty([12 * 471, 18 * 9], dtype = float)\n",
    "# y是output，共12*471笔data，每个output是第10个小时的第9种大气成分PM2.5，因此只有1维\n",
    "y = np.empty([12 * 471, 1], dtype = float)\n",
    "# 取出input和output\n",
    "# 每个月份共471笔data，依次遍历即可\n",
    " # 用一个18行(18种物质),10列(10个小时的数据，包括input 18行(18种物质),9列(9个小时) 变成9 * 18 维的一行\n",
    " # 和output18行(18种物质),1列(1个小时)) 来存放每一笔Training data\n",
    "for month in range(12):\n",
    "    for day in range(20):\n",
    "        for hour in range(24):\n",
    "            # 19天 15h 代表本月取完，开始取下个月\n",
    "            if day == 19 and hour > 14:\n",
    "                continue\n",
    "            x[month * 471 + day * 24 + hour, :] = month_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1, -1) #vector dim:18*9 (9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9)\n",
    "             # month_data前9列的作为input的数据(18行9列)用reshape平摊到一个18*9的行向量上\n",
    "             # month_data的第10列作为output，只有第10行的PM2.5值是有用的\n",
    "            y[month * 471 + day * 24 + hour, 0] = month_data[month][9, day * 24 + hour + 9] #value\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wOii0TX8IwE"
   },
   "source": [
    "# **Normalize (1)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceMqFoNI8ftQ"
   },
   "outputs": [],
   "source": [
    "# 这个时候我们已经有了Training data的x和y，但由于feature的每一个dimension大小范围都是不一样的，因此还要对其做feature scale\n",
    "# 求出期望mean和标准差std，利用公式x'=(x-u)/σ来使同一列的数据同时满足同一个分布\n",
    "mean_x = np.mean(x, axis = 0) #18 * 9 \n",
    "std_x = np.std(x, axis = 0) #18 * 9 \n",
    "for i in range(len(x)): #12 * 471\n",
    "    for j in range(len(x[0])): #18 * 9 \n",
    "        if std_x[j] != 0:\n",
    "            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]\n",
    "print(mean_x.shape)\n",
    "print(std_x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzvXP5Jya64j"
   },
   "source": [
    "# **Split Training Data Into \"train_set\" and \"validation_set\"**\n",
    "这部分是针对作业中 report 的第二题、第三题做的简单示范，以生成比较中用来训练的 train_set 和不会被放入训练、只是用来验证的 validation_set。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feF4XXOQb5SC"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "x_train_set = x[: math.floor(len(x) * 0.8), :]\n",
    "y_train_set = y[: math.floor(len(y) * 0.8), :]\n",
    "x_validation = x[math.floor(len(x) * 0.8): , :]\n",
    "y_validation = y[math.floor(len(y) * 0.8): , :]\n",
    "print(x_train_set)\n",
    "print(y_train_set)\n",
    "print(x_validation)\n",
    "print(y_validation)\n",
    "print(len(x_train_set))\n",
    "print(len(y_train_set))\n",
    "print(len(x_validation))\n",
    "print(len(y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-qAu0KR_ZRR"
   },
   "source": [
    "# **Training**\n",
    "![alt text](https://drive.google.com/uc?id=1xIXvqZ4EGgmxrp7c9r0LOVbcvd4d9H4N)\n",
    "![alt text](https://drive.google.com/uc?id=1S42g06ON5oJlV2f9RukxawjbE4NpsaB6)\n",
    "![alt text](https://drive.google.com/uc?id=1BbXu-oPB9EZBHDQ12YCkYqtyAIil3bGj)\n",
    "\n",
    "(和上图不同处: 下面的 code 采用 Root Mean Square Error)\n",
    "\n",
    "因为常数项的存在，所以 dimension (dim) 需要多加一栏；eps 项是避免 adagrad 的分母为 0 而加的极小数值。\n",
    "\n",
    "每一个 dimension (dim) 会对应到各自的 gradient, weight (w)，透过一次次的 iteration (iter_time) 学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCzDfxBFBFqp"
   },
   "outputs": [],
   "source": [
    "# 现在开始尝试简单的linear Regression: y = w_1*x_1+w_2*x_2+...+b\n",
    "# 由于常数项的存在，dimension需要在18*9的基础上再加上一维，初始化每一维对应的weight为0，并进行gradient descent，经过多次iteration来学习\n",
    "dim = 18 * 9 + 1\n",
    "w = np.zeros([dim, 1])\n",
    "x = np.concatenate((np.ones([12 * 471, 1]), x), axis = 1).astype(float)\n",
    "# 利用np.concatenate函数在原先的x上加上一个常数的feature=1,用一个全1的列向量，并使用axis=1来横向拼接\n",
    "learning_rate = 100\n",
    "iter_time = 1000\n",
    "adagrad = np.zeros([dim, 1])\n",
    "# adagrad表达式为w'=w-lr*gd/sqrt(Σgd^2))，为了避免分母为0，加上一个很小的eps\n",
    "eps = 0.0000000001\n",
    "# loss function = (x*w-y)^2, loss对w求导为 2*x*(x*w-y),即gradient\n",
    "for t in range(iter_time):\n",
    "    loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))/471/12)#rmse\n",
    "    if(t%100==0):\n",
    "        print(str(t) + \":\" + str(loss))\n",
    "    gradient = 2 * np.dot(x.transpose(), np.dot(x, w) - y) #dim*1\n",
    "    adagrad += gradient ** 2\n",
    "    # 更新w,注意这里gradient、adagrad都是列向量，加减乘除的用法跟常数一样\n",
    "    w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
    "np.save('weight.npy', w)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqNdWKsYBK28"
   },
   "source": [
    "# **Testing**\n",
    "![alt text](https://drive.google.com/uc?id=1165ETzZyE6HStqKvgR0gKrJwgFLK6-CW)\n",
    "\n",
    "加载 test data，并且以相似于训练数据预先处理和特征萃取的方式处理，使 test data 形成 240 个维度为 18 * 9 + 1 的资料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AALygqJFCWOA"
   },
   "outputs": [],
   "source": [
    "# testdata = pd.read_csv('gdrive/My Drive/hw1-regression/test.csv', header = None, encoding = 'big5')\n",
    "testdata = pd.read_csv('./test.csv', header = None, encoding = 'big5')\n",
    "test_data = testdata.iloc[:, 2:]\n",
    "test_data[test_data == 'NR'] = 0\n",
    "test_data = test_data.to_numpy()\n",
    "test_x = np.empty([240, 18*9], dtype = float)\n",
    "for i in range(240):\n",
    "    test_x[i, :] = test_data[18 * i: 18* (i + 1), :].reshape(1, -1)\n",
    "for i in range(len(test_x)):\n",
    "    for j in range(len(test_x[0])):\n",
    "        if std_x[j] != 0:\n",
    "            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]\n",
    "test_x = np.concatenate((np.ones([240, 1]), test_x), axis = 1).astype(float)\n",
    "test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJQks9JEHR6W"
   },
   "source": [
    "# **Prediction**\n",
    "说明图同上\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1165ETzZyE6HStqKvgR0gKrJwgFLK6-CW)\n",
    "\n",
    "有了 weight 和测试数据即可预测 target。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNyB229jHsEQ"
   },
   "outputs": [],
   "source": [
    "w = np.load('weight.npy')\n",
    "ans_y = np.dot(test_x, w)\n",
    "ans_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKMKW7RzHwuO"
   },
   "source": [
    "# **Save Prediction to CSV File**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dwfpqqy0H8en"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('submit.csv', mode='w', newline='') as submit_file:\n",
    "    csv_writer = csv.writer(submit_file)\n",
    "    header = ['id', 'value']\n",
    "    print(header)\n",
    "    csv_writer.writerow(header)\n",
    "    for i in range(240):\n",
    "        row = ['id_' + str(i), ans_y[i][0]]\n",
    "        csv_writer.writerow(row)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y54yWq9cIPR4"
   },
   "source": [
    "相关 reference 可以参考:\n",
    "\n",
    "Adagrad :\n",
    "https://youtu.be/yKKNr-QKz2Q?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&t=705 \n",
    "\n",
    "RMSprop : \n",
    "https://www.youtube.com/watch?v=5Yt-obwvMHI \n",
    "\n",
    "Adam\n",
    "https://www.youtube.com/watch?v=JXQT_vxqwIs \n",
    "\n",
    "\n",
    "以上 print 的部分主要是为了看一下资料和结果的呈现，拿掉也无妨。另外，在自己的 linux 系统，可以将档案写死的的部分换成 sys.argv 的使用 (可在 terminal 自行输入档案和档案位置)。\n",
    "\n",
    "最后，可以藉由调整 learning rate、iter_time (iteration 次数)、取用 features 的多寡(取几个小时，取哪些特征字段)，甚至是不同的 model 来超越 baseline。\n",
    "\n",
    "Report 的问题模板请参照 : https://docs.google.com/document/d/1s84RXs2AEgZr54WCK9IgZrfTF-6B1td-AlKR9oqYa4g/edit"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw1_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}